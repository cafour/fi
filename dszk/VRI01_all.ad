= Interaction in VR
:url: ./vr_interaction/
:page-group: vri
:page-order: VRI01

Human-centered interaction design::
Focuses on the human side of human-computer interaction -- the interface from the user's point of view. In the ideal case, VR experiences should be not only effective, but also engaging and enjoyable.

Intuitiveness::
* An intuitive interface can be quickly understood, accurately predicted, and easily used.
* Intuitiveness in the mind of the user but the interface and the world can be designed to help form a useful mental model.
* Exploit _interaction metaphors_ that the users are likely already familiar with.
* In VR, the users are cut off and it's thus even less likely that they will read a manual or talk to someone.

Break-in-presence::
When the illusion of being in virtual world is broken and the user realizes they are in the real world wearing a headset. Can be caused by, for example, rendering issues, loss of tracking, tripping over a wire, people talking IRL,etc.

Proprioception::
Sensation of limb and body pose and motion. Derived from nerves in muscles, tendons, joints, etc. It's what allows us to touch our nose even with eyes closed. Can be leveraged in VR as, for example, users don't need to see the tools in their hands to use them.

Gorilla arm::
Arm fatigue caused by extended usage of gesture-based interfaces. Typically happens when the user must hold hands above waist for more than 10 seconds.

Clutching::
Having to release and re-grasp an object to complete a task because it's impossible to do in one motion (e.g., wrench).

== Norman's principles of interaction design

Don Norman::
* American researcher, professor, and author.
* Wrote _The Design of Everyday Things_.
* Expert on design, usability engineering, and cognitive science.
* Advocate of _user-centered design_.

Cycle of interaction::
High-quality interaction design considers tasks, requirements, intentions, and desires at each of this seven stages:
+
--
1. Forming the **goal** (one stage)
2. **Executing** the action
** Bridges the gap between goal and result.
** Three stages:
*** _plan_ -- What are my options of accomplishing my goals?
*** _specify_ -- What is the specific sequence of actions I have to do?
*** _perform_ -- Do the action.
** The user should be able to anticipate the result (_feedforward_) based on signifiers, constraints, mappings, and their mental model.

3. **Evaluating** the results
** Allows the user to judge whether their goal has been achieved, make adjustments, and form new goals.
** Three stages:
*** _perceive_ -- What happened?
*** _interpret_ -- What does it mean?
*** _compare_ -- Is this what I wanted?
** _Feedback_ is obtained by perceiving the impact of the action.
--
+
image::./img/vri01_cycle_of_interaction.png[]
+
.Moving a boulder
====
**Goal:** Move a boulder blocking the path.

**Plan:** Go to the boulder (or select it from a distance) and move it.

**Specify:** Shoot a ray from the hand, intersect the boulder, push the _grab_ button, move the hand, release the _grab_ button.

**Perfom:** Move the boulder.

**Perceive:** The boulder is now at a new location.

**Interpret:** The boulder is no longer in the path.

**Compare**: I have cleared the route and can continue on my path.
====

Conscious and subconscious thought in the cycle of interaction::
* Goals tend to be _reflective_ (slow conscious thought).
* We are often unaware of the execution and evaluation stages unless we encounter an obstacle.
** _Specify_ and _interpret_ are often semi-conscious.
** _Perform_ and _perceive_ are typically automatic and subconscious.

Gulf of execution::
The goal is known, but the path to it isn't.

Gulf of evaluation::
When we don't understand the results of an action.

Goal-driven behavior::
The user themselves have initiated the interaction cycle by setting a goal for themselves.

Data-driven / event-driven behavior::
Something happened in the world. The goal is _opportunistic_. It's less precise, less certain, but also require less mental effort.

Affordances::
Relationships that define what actions are possible and how something can be interacted with by a user.
+
--
* "to afford" = "to offer" / "to make possible"
* Relationship between the properties of an object and the capabilities of an agent (typically a human user).
** _Example: Light switch on a wall can toggle the light but only for those that can reach it._
** _Example: Glass affords transparency for non-blind people. Glass doesn't afford passage of solid objects._
** _Example: Buttons can be pressed by people with hands._
* They don't have to be perceivable to exist.
--

Signifiers::
Perceivable indicators that communicate the purpose, structure, operation, and behavior of an object to a user (i.e., make certain affordances perceivable).
+
--
* Informs the user what is possible before they interact.
* Examples: signs, labels, images, feel of a button on a controller, etc.
* _Misleading signifiers_ can be ambiguous or represent an affordance that doesn't actually exist.
* Can be unintentional but useful. For example, garbage on a beach represents unhealthy conditions.
** Example: Something looks like a drawer although it cannot be opened.
** Can be intentional and purposeful. For example, to motivate the user to find a key to the drawer.
* Do not have to be attached to an object. Can signify general information, such as current interaction mode.
--

Constraints::
Limitations of actions and behavior.
+
--
* Signifiers should be used to make constraints easy to understand.
* If communicated well, they can improve accuracy, precision, and user efficiency.
* Should be consistent, so that learning can be transferred across tasks.
* Experts should be able to disable constraints.
* In VR, they are typically physical or mathematical, but in general can be also be semantic or even cultural.
* _Degrees of Freedom (DoF)_ -- Number of independent dimensions available for motion of an entity.
** 6 DoF -- 3 axes for movement, 3 axes for rotation
** _Example: A UI slider has 1 DoF. It's constrained to move either left or right._
* Constraints can add realism (e.g., preventing users from clipping through walls) but can also make interactions more difficult.
** _Example: It can be useful to leave virtual tools hanging in the air instead of picking them up from the ground._
--

Feedback::
Communicates results of an action or task status. Helps the user understand current state of things and drive future action.
+
--
* Lack of immediate visual feedback when the user moves can result in break-in-presence or even motion sickness.
* Haptic feedback is especially difficult to implement (e.g., real forces caused by walls) but _sensory substitution_ can be used.
* Should be prioritized based on importance to avoid overwhelming the user.
* Instead of putting stuff on a heads-up display (or near the head), consider putting it near the waist. There, it can be easily accessed but isn't obtrusive.
* Users should be allowed to configure or disable it.
--

Sensory substitution::
Replacement of an ideal sensory cue, which is unavailable, with one or more other sensory cues.
+
--
* _Ghosting_ -- Second rendering of an object in a different pose.
* _Highlighting_ -- Visually outlining an object.
* _Audio cues_ -- For example, sounds that signify collision.
* _Passive haptics_ -- Static objects IRL that can be touched.
* _Rumble_ -- Vibration of input devices.
--


Mappings::
Correlation between two sets of things. Typical example is a set of inputs controling a corresponding set of actions.
+
--
* Mappings are useful even when the thing cannot be manipulated with directly (e.g., using a broom to flip a switch).
* Input devices have a mapping natural for some interaction pattern but poor for another.
** _Example: Hand-tracked devices are good for pointing, not so much as a steering wheel._
* _Non-spatial mappings_ transform spatial input to non-spatial and vice versa.
** _Example: Raising hand means "more". Lowering hand means "less"._
** Some are universal, other may be personal or culture dependent.
--

Compliance::
Matching of sensory feedback with input devices across time and space. Results in perceptual binding -- feeling of interacting with one coherent object.
+
--
* _Spatial compliance_ -- Direct mappings in space lead immediately to understanding.
** _Position compliance_ -- Co-location of sensory feedback and input device position. For example, when we see our hand in VR where we would expect based on our proprioception.
** _Directional compliance_ -- Objects in VR should move and rotate as the manipulated input device. It's the most important spatial compliance. For example, mouse and cursor.
** _Nulling compliance_ -- When an input device is returned to its initial placement, its VR object is too. Allows the user to use their "muscle memory" to remember the initial object placements.
* _Temporal compliance_ -- Different types of feedback (e.g., proprioceptive and visual) to the same action should be synced.
** _Visio-vestibular compliance_ -- Viewpoint feedback should be immediate. When we move our head, we should see and perceive with the inner ear what we'd expect. Especially important to avoid motion sickness.
** If action cannot be completed immediately, there should be some kind of feedback implying that the action is in progress. However, slow of poor feedback is more frustrating than no feedback.
--


== Modes and flow

Modes::
Complex applications with different types of tasks may require different interaction techniques. One or more settings that change the available interaction techniques.
+
--
* Current mode can be changed by pressing a button, selecting it in a UI panel, or it may be order-dependent (i.e., activate automatically based on previous actions).
* The current mode should always be made clear to the user.
--

Flow::
Seamless integration of various tasks and techniques provided by the application.
+
--
* _Example: An object should be selected first, before an action can be done upon it._
* Sequence of interactions should occur without distractions.
* Users should not have to physically (e.g., eyes, hands, head) or cognitively move between tasks.
* Lightweight mode switching, physical props, and multimodal techniques help maintain flow.
--

Object-action vs action-object::
People prefer to first think about an object and then an action done upon/with the object.
+
--
* In natural language, action-object is more common (e.g., "Pick up the book.")
* Object is concrete an easier to think about.
* Action is abstract. It's easier to imagine the action with an object already in mind.
--

Multimodal interaction::
When multiple input and output sensory modalities are available for accomplishing the same thing. Individual to both the application and the users.
+
--
* It's better the use a specialized modality when it's clearly the best (e.g., selection by pointing for your app).
* When user preference is divided, it may be better to implement both options.
--


== Selection patterns

Hand selection pattern::
* When selection needs to be realistic.
* Can simulate entire arms, or just hands.
* _Go-go technique_ -- When extended beyond 2/3 of reach, the hand grows non-linearly, allowing to pick up far-away objects.

Pointing pattern::
* Eye tracking based on dwell selection (looking for a period of time) can be frustrating. Don't use unless necessary.
+
TODO

Image-plane selection pattern::
* Don't use when selection is frequent, because of gorilla arm.
+
TODO

Volume-based selection pattern::
* Good for selecting data without geometric surfaces.
* Can be difficult for novice users.
+
TODO


== Manipulation patterns

Direct hand manipulation pattern::
* More efficient and satisfying than other manipulation patterns.
* For high fidelity, use in combination with hand selection.
* Non-isomorphic rotations can reduce clutching and increase performance and precision.
+
TODO

Proxy pattern::
* Useful for remote manipulation or when the world or the user themself is scaled.
* Use tracked physical props for natural two-handed interactions and tactile feedback.
+
TODO

3D tool pattern::
* Enhances manipulation capability of the hands.
* Use signifiers for object-attached tools to make it obvious how it's used.
* Use _jigs_ for user-controlled contraints and snapping.
+
TODO

== Compound patterns

World-in-Miniature (WiM)::
* Provides situational awareness, user-defined proxies, and quick movement.
* Use maps where forward is always up so that the orientation of the map matches the orientation of the world.
** Allow disabling.
* Don't map the user's doll's movement to the user's perspective (causes motion sickness), teleport instead.
+
TODO


== Questions

====

How can it be applied to SoftVis in VR?::
TODO

What should I think about from the design perspective?::
TODO

====

== Sources

* Jason Jerald, The VR Book: Human-Centered Design for Virtual Reality, 2016
