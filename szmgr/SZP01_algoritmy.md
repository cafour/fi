---
title: "Algoritmy a datovÃ© struktury"
description: "TODO"
---

<dl><dt><strong>ğŸ“Œ NOTE</strong></dt><dd>

PokroÄilÃ© techniky nÃ¡vrhu algoritmÅ¯: dynamickÃ© programovÃ¡nÃ­, hladovÃ© strategie, backtracking. AmortizovanÃ¡ analÃ½za. VyhledÃ¡vÃ¡nÃ­ Å™etÄ›zcÅ¯: naivnÃ­ algoritmus pro hledÃ¡nÃ­ Å™etÄ›zcÅ¯, Karp-RabinÅ¯v algoritmus, hledÃ¡nÃ­ Å™etÄ›zcÅ¯ pomocÃ­ koneÄnÃ½ch automatÅ¯. Algoritmus Knuth-Morris-Pratt.

_IV003_

</dd></dl>

## PokroÄilÃ© techniky nÃ¡vrhu algoritmÅ¯

### DynamickÃ© programovÃ¡nÃ­

> I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.
>
> â€” Richard Bellman

IntutivnÄ› je _dynamickÃ© programovÃ¡nÃ­_ spojenÃ­m dvou vÄ›cÃ­: "rozbalenÃ©" rekurze (taky se tomu Å™Ã­kÃ¡ _bottom-up pÅ™Ã­stup_) a _memoizace_.

- Je pouÅ¾itelnÃ© na problÃ©my, kterÃ© lze rozdÄ›lit na podproblÃ©my.
- ObzvlÃ¡Å¡Å¥ vhodnÃ© je pak v tÄ›ch pÅ™Ã­padech, kde se podproblÃ©my pÅ™ekrÃ½vajÃ­ -- dochÃ¡zÃ­ k tomu, Å¾e se nÄ›co poÄÃ­tÃ¡ vÃ­ckrÃ¡t.

KonkrÃ©tnÄ›ji, dynamickÃ© programovÃ¡nÃ­ je vhodnou technikou, pokud:

- podproblÃ©mÅ¯ je polynomiÃ¡lnÃ­ poÄet,
- (optimÃ¡lnÃ­) Å™eÅ¡enÃ­ pÅ¯vodnÃ­ho problÃ©mu lze jednoduÅ¡e spoÄÃ­tat z (optimÃ¡lnÃ­ch) Å™eÅ¡enÃ­ jeho podproblÃ©mÅ¯,
- podproblÃ©my jde pÅ™irozenÄ› seÅ™adit od _nejmenÅ¡Ã­ho_ po _nejvÄ›tÅ¡Ã­_.

<dl><dt><strong>ğŸ’¡ TIP</strong></dt><dd>

O tom, Å¾e problÃ©mÅ¯ musÃ­ bÃ½t polynomiÃ¡lnÃ­ poÄet, pÅ™emÃ½Å¡lÃ­m intuitivnÄ› tak, Å¾e se musÃ­ dÃ¡t vyÅ™eÅ¡it v nÄ›jakÃ©m vÃ­cenÃ¡sobnÃ©m `for`-cyklu a uloÅ¾it do multi-dimenzionÃ¡lnÃ­ho pole.

Pokud mÃ¡m $l$ zanoÅ™enÃ½ch cyklÅ¯, vyÅ™eÅ¡Ã­m nejvÃ­c $n^l$ podproblÃ©mÅ¯.

</dd></dl>

#### Memoizace

_Memoizace_ v zÃ¡sadÄ› nenÃ­ nic jinÃ©ho neÅ¾ tabulka, pole, `HashSet`, nebo nÄ›co podobnÃ©ho, kam si algoritmus uklÃ¡dÃ¡ Å™eÅ¡enÃ­ jednotlivÃ½ch podproblÃ©mÅ¯.

**ğŸ’¡ TIP**\
V pseudokÃ³du se oznaÄuje jako $M$ (asi memory), $A$ (asi array), nebo $C$ (asi cache).

#### Bottom-up

Rekurze tradiÄnÄ› Å™eÅ¡Ã­ problÃ©m _zeshora_ -- zaÄnÄ› celÃ½m problÃ©mem, kterÃ½ si rozdÄ›lÃ­ na podproblÃ©my, a ty na podpodproblÃ©my, atd. Bottom-up approach jde na to obrÃ¡cenÄ›. ZaÄnÄ› tÄ›mi nejmenÅ¡Ã­mi podproblÃ©my a postupnÄ› se prokousÃ¡vÃ¡ k reÅ¡enÃ­ celku.

JedinÃ½m hÃ¡Äek je v tom pÅ™ijÃ­t na to, kterÃ© podproblÃ©my jsou ty nejmenÅ¡Ã­ a v jakÃ©m poÅ™Ã¡dÃ­ je musÃ­me spoÄÃ­tat, aby byly vÅ¡echny pÅ™ipravenÃ© pro vÃ½poÄet vÄ›tÅ¡Ã­ch podproblÃ©mÅ¯. Bez tohohle algoritmus nebude fungovat korektnÄ›.

<dl><dt><strong>ğŸ“Œ NOTE</strong></dt><dd>

ZjednoduÅ¡enÄ› jde o to pÅ™etransformovat rekurzi na cykly. PÄ›knÃ½ vedlejÅ¡Ã­m efektem je, Å¾e je jednoduÅ¡Å¡Ã­ urÄit sloÅ¾itost algoritmu.

</dd></dl>

#### KuchaÅ™ka

1. RozdÄ›l problÃ©m na (pÅ™ekrÃ½vajÃ­cÃ­ se) podproblÃ©my.
2. NapiÅ¡ rekurzivnÃ­ algoritmus nebo alespoÅˆ BellmanÅ¯v rekurentnÃ­ vztah (znaÄenÃ½ $\text{OPT}$ protoÅ¾e dÃ¡vÃ¡ _optimÃ¡lnÃ­_ Å™eÅ¡enÃ­).
3. UrÄi sprÃ¡vnÃ© poÅ™adÃ­ poÄÃ­tÃ¡nÃ­ podproblÃ©mÅ¯ tak, aby se kaÅ¾dÃ½ poÄÃ­tal prÃ¡vÄ› jednou (bottom-up pÅ™Ã­stup).
4. Pokud je to nutnÃ©, sestav z optimÃ¡lnÃ­ hodnoty jejÃ­ realizaci (tÅ™eba cestu nebo nÄ›co).
5. SepiÅ¡ pseudokÃ³d.
6. DokaÅ¾ korektnost rekurentnÃ­ho vztahu, bottom-up poÅ™adÃ­ a rekonstrukce (zejmÃ©na terminace).
7. Okomentuj sloÅ¾itost.

#### ProblÃ©my

- **Weighted interval scheduling**\
  Z mnoÅ¾iny $n$ intervalÅ¯ (udÃ¡lostÃ­, ÃºkolÅ¯, atd.), kterÃ© se mohou pÅ™ekrÃ½vat v Äase, a majÃ­ urÄitou vÃ¡hu $w_i$, vyber takovou mnoÅ¾inu intervalÅ¯ $S$, pro kterou je $\sum_{i \in S} w_s$ maximÃ¡lnÃ­.

  - **Å˜eÅ¡enÃ­**

    Å˜eÅ¡enÃ­ vyuÅ¾Ã­vÃ¡ toho, Å¾e Äas plyne vÃ½hradnÄ› dopÅ™edu, takÅ¾e se mÅ¯Å¾eme na podproblÃ©my dÃ­vat chronologicky a nebudou se pÅ™ekrÃ½vat.

    NechÅ¥ $p(j)$ je index takovÃ© udÃ¡losti $i &lt; j$, Å¾e $i$ a $j$ jsou kompatibilnÃ­.

    ```math
    \text{OPT}(j) = \begin{cases}

    0 & \text{pokud } j = 0 \\
    \max \{ \text{OPT}(j-1), w_j + \text{OPT}(p(j)) \} & \text{pokud } j > 0

    \end{cases}
    ```

- **Parenthesization**\
  MÄ›jme hromadu matic, kterÃ© chceme pronÃ¡sobit. VÃ­me, Å¾e maticovÃ© nÃ¡sobenÃ­ je asociativnÃ­, takÅ¾e mÅ¯Å¾eme zvolit rÅ¯znÃ© poÅ™adÃ­ nÃ¡sobenÃ­ -- rÅ¯znÃ© odzÃ¡vorkovÃ¡nÃ­. NicmÃ©nÄ›, nenÃ­ komutativnÃ­, takÅ¾e nesmÃ­me matice prohazovat. Cena nÃ¡sobenÃ­ matice o velikosti $i \times j$ a $j \times k$ je $i \cdot j \cdot k$. JakÃ© poÅ™adÃ­ zvolit, aby byl vÃ½slednÃ½ souÄin co nejlevnÄ›jÅ¡Ã­?

  - **ProblÃ©m**

    MÃ¡me matice $A_1, A_2, ..., A_n$, kterÃ© chceme pronÃ¡sobit.

    PotÅ™ebujeme najÃ­t index $k$ takovÃ½, Å¾e $\textcolor{red}{(A_1 \cdot ... \cdot A_k)} \cdot \textcolor{blue}{(A_{k+1} \cdot ... \cdot A_n)}$ je nefektivnÄ›jÅ¡Ã­. To nÃ¡m problÃ©m rozdÄ›luje na dva podproblÃ©my: ÄervenÃ½ a modrÃ½.

  - **Å˜eÅ¡enÃ­**

    ```math
    \text{OPT}(i, j) = \begin{cases}

    0 & \text{pokud } i = j \\
    \min_{i \leq k < j} \{ \text{OPT}(i, k) + \text{OPT}(k+1, j) + p_{i-1} \cdot p_k \cdot p_j \} & \text{pokud } i < j

    \end{cases}
    ```

- **Knapsack**\
  MÄ›jme batoh s nosnostÃ­ $W$ a $n$ vÄ›cÃ­, kterÃ© bychom do nÄ›j rÃ¡di naloÅ¾ili. KaÅ¾dÃ¡ vÄ›c $i$ mÃ¡ hodnotu $v_i$ a vÃ¡hu $w_i$. JakÃ© vÄ›ci vybrat, aby byla hodnota naloÅ¾enÃ½ch vÄ›cÃ­ co nejvÄ›tÅ¡Ã­, ale batoh je furt unesl?

  - **Å˜eÅ¡enÃ­**

    VychÃ¡zÃ­ z myÅ¡lenky, Å¾e batoh, ve kterÃ©m uÅ¾ nÄ›co je, je _jakoby_ batoh s niÅ¾Å¡Ã­ nosnostÃ­.

    ProchÃ¡zÃ­me vÄ›ci postupnÄ› pÅ™es index $i$ a pro kaÅ¾dou Å™eÅ¡Ã­me, jestli ji chceme v batohu o nosnosti $w$:

    ```math
    \text{OPT}(i, w) = \begin{cases}

    0 & \text{pokud } i = 0 \\
    \text{OPT}(i - 1, w) & \text{pokud } w_i > w \\
    \max \{ \text{OPT}(i - 1, w), v_i + \text{OPT}(i - 1, w - w_i) \} & \text{pokud } w_i \leq w

    \end{cases}
    ```

### HladovÃ© (greedy) strategie

> PÅ™ijde Honza na pracovnÃ­ pohovor a budoucÃ­ Å¡Ã©f se ho ptÃ¡: "Co je vaÅ¡e dobrÃ¡ schopnost?"
> Honza odpovÃ­: "UmÃ­m rychle poÄÃ­tat."
> "Kolik je 1024 na druhou?"
> "MILION STO TISÃC," vyhrkne ze sebe Honza.
> Å Ã©f se chvÃ­li zamyslÃ­ a povÃ­dÃ¡: "Ale to je Å¡patnÄ›, vÃ½sledek je 1048576!"
> A Honza na to: "No sice Å¡patnÄ›, ale sakra rychle!"

Greedy algoritmy nachÃ¡zÃ­ Å™eÅ¡enÃ­ globÃ¡lnÃ­ho problÃ©mu tak, Å¾e volÃ­ lokÃ¡lnÄ› optimÃ¡lnÃ­ Å™eÅ¡enÃ­. Tahle taktika nemusÃ­ vÃ©st ke globÃ¡lnÄ› optimÃ¡lnÃ­mu Å™eÅ¡enÃ­, ale alespoÅˆ ho spoÄÃ­tÃ¡ rychle.

- Ve vÃ½poÄtu smÄ›Å™uje bottom-up.
- IdeÃ¡lnÄ› funguje na problÃ©my, kde optimÃ¡lnÃ­ Å™eÅ¡enÃ­ podproblÃ©mu je souÄÃ¡stÃ­ optimÃ¡lnÃ­ho Å™eÅ¡enÃ­ celÃ©ho problÃ©mu.
- DobÅ™e se navrhuje, Å¡patnÄ› dokazuje.

#### ProblÃ©my

- **Cashierâ€™s algorithm (mince)**\
  Jak zaplatit danou ÄÃ¡stku s co nejmenÅ¡Ã­m poÄtem mincÃ­ rÅ¯znÃ½ch hodnot?

  - **Å˜eÅ¡enÃ­**\
    V kaÅ¾dÃ© iteraci vol minci s nejvyÅ¡Å¡Ã­ hodnotou, dokud nenÃ­ zaplacena celÃ¡ ÄÃ¡stka.

- **Interval scheduling**\
  Z mnoÅ¾iny intervalÅ¯, kterÃ© majÃ­ zaÄÃ¡tek a konec, **ale majÃ­ stejnou hodnotu**, vyber nejvÄ›tÅ¡Ã­ podmnoÅ¾inu intervalÅ¯, kterÃ© se nepÅ™ekrÃ½vajÃ­.

  - **Å˜eÅ¡enÃ­**\
    Vybereme ty, kterÃ© konÄÃ­ nejdÅ™Ã­v.

### Backtracking

_InteligentnÃ­ brute-force nad prostorem Å™eÅ¡enÃ­._

Technika hledÃ¡nÃ­ Å™eÅ¡enÃ­ problÃ©mu postupnÃ½m sestavovÃ¡nÃ­m _kandidÃ¡tnÃ­ho_ Å™eÅ¡enÃ­. [backtracking](#backtracking)

- ÄŒÃ¡steÄnÃ½ kandidÃ¡t mÅ¯Å¾e bÃ½t zavrhnut, pokud nemÅ¯Å¾e bÃ½t dokonÄen.
- MÅ¯Å¾eme dokonce zavrhnout kompletnÃ­ Å™eÅ¡enÃ­, pokud je chceme najÃ­t vÅ¡echna.
- Pokud je kandidÃ¡t zavrhnut, algoritmus se vrÃ¡tÃ­ o kus zpÄ›t (backtrackuje), upravÃ­ parametry a zkusÃ­ to znovu.

**PorovnÃ¡nÃ­ s dynamickÃ½m programovÃ¡nÃ­m**

| DynamickÃ© programovÃ¡nÃ­                                       |
| ------------------------------------------------------------ |
| Backtracking                                                 |
| HledÃ¡ Å™eÅ¡enÃ­ _pÅ™ekrÃ½vajÃ­cÃ­ch se podproblÃ©mÅ¯_.                |
| HledÃ¡ _vÅ¡echna_ Å™eÅ¡enÃ­.                                      |
| HledÃ¡ _optimÃ¡lnÃ­_ Å™eÅ¡enÃ­.                                    |
| HledÃ¡ vÅ¡echna, _libovolnÃ¡_ Å™eÅ¡enÃ­, _hrubou silou_.           |
| MÃ¡ blÃ­Å¾ k BFS -- stavÃ­ "vrstvy".                             |
| MÃ¡ blÃ­Å¾ k DFS -- zanoÅ™Ã­ se do jednoho Å™eÅ¡enÃ­ a pak se vrÃ¡tÃ­. |
| Typicky zabÃ­rÃ¡ vÃ­c pamÄ›ti kvÅ¯li memoizaci.                   |
| Typicky trvÃ¡ dÃ©le, protoÅ¾e hledÃ¡ _vÅ¡echna_ Å™eÅ¡enÃ­.           |
| MÃ­vÃ¡ cykly.                                                  |
| MÃ­vÃ¡ rekurzi.                                                |

#### ProblÃ©my

- **Sudoku**\
  HledÃ¡ Å™eÅ¡enÃ­ tak, Å¾e pro pole vybere moÅ¾nÃ© Å™eÅ¡enÃ­ a zanoÅ™Ã­ se, pokud funguje tak _hurÃ¡_, pokud ne, tak backtrackuje a zkusÃ­ jinou moÅ¾nou cifru.
- **Eight queens**\
  Jak rozestavit osm Å¡achovÃ½ch krÃ¡loven na Å¡achovnic tak, aby se vzÃ¡jemnÄ› neohroÅ¾ovaly?

## AmortizovanÃ¡ analÃ½za

> - **_amortize(v)_**
>   - _amortisen_ -- "to alienate lands", "to deaden, destroy"
>   - _amortir_ (Old French) -- "deaden, kill, destroy; give up by right"
>   - _\*admortire_ (Vulgar Latin) -- to extinquish
>
> â€” Online Etymology Dictionary

UmoÅ¾Åˆuje pÅ™esnÄ›jÅ¡Ã­ analÃ½zu ÄasovÃ© a prostorovÃ© sloÅ¾itosti, protoÅ¾e uvaÅ¾ujeme kontext, ve kterÃ© se analyzovanÃ½ algoritmus pouÅ¾Ã­vÃ¡. UrÄujeme sloÅ¾itost operace v **posloupnosti operacÃ­**, **ne samostatnÄ›**.

**PÅ™ipomenutÃ­**

**ğŸ’¡ TIP**\
Viz bakalÃ¡Å™skÃ¡ otÃ¡zka [Korektnost a sloÅ¾itost algoritmu](../../szb/korektnost-a-slozitost-algoritmu/).

ZÃ¡kladnÃ­mi pojmy analÃ½zy sloÅ¾itosti jsou:

- **ÄŒasovÃ¡ sloÅ¾itost**\
  Funkce velikosti vstupu $n$ algoritmu. PoÄÃ­tÃ¡ poÄet _krokÅ¯_ (nÄ›jakÃ© vÃ½poÄetnÃ­ jednotky) potÅ™ebnÃ½ch k vyÅ™eÅ¡enÃ­ problÃ©mu.
- **ProstorovÃ¡ sloÅ¾itost**\
  Funkce velikosti vstup $n$ algoritmu. PoÄÃ­tÃ¡ poÄet _polÃ­_ (nÄ›jakÃ© jednotky prostoru), kterÃ¡ algoritmus potÅ™ebuje navÅ¡tÃ­vit k vyÅ™eÅ¡enÃ­ problÃ©mu.
- **AsymptotickÃ¡ notace**\
  UmoÅ¾Åˆuje zanedbat hardwarovÃ© rozdÃ­ly. Popisuje, Å¾e sloÅ¾itost roste _alespoÅˆ tak_, _nejvÃ½Å¡ tak_ nebo _stejnÄ›_ jako jinÃ¡ funkce.
- **Big O**\
  HornÃ­ mez, sloÅ¾itost v nejhorÅ¡Ã­m pÅ™Ã­padÄ›. MnoÅ¾ina funkcÃ­ rostoucÃ­ch stejnÄ› rychle jako $g$, nebo **pomaleji**:

  ```math
  \mathcal{O}(g(n)) = \{f : (\exists c, n_0 \in \mathbb{N}^+)(\forall n \geq n_0)(f(n) \le c \cdot g(n)) \}
  ```

- **Omega**\
  SpodnÃ­ mez, sloÅ¾itost v nejlepÅ¡Ã­m pÅ™Ã­padÄ›. MnoÅ¾ina funkcÃ­ rostoucÃ­ch stejnÄ› rychle jako $g$, nebo **rychleji**.

  ```math
  \Omega(g) = \{f : (\exists c, n_0 \in \mathbb{N}^+)(\forall n \geq n_0)(f(n) \ge c \cdot g(n)) \}
  ```

- **Theta**\
  HornÃ­ i spodnÃ­ mez. MnoÅ¾ina funkcÃ­ rostoucÃ­ch stejnÄ› rychle jako $g$.

  ```math
  \Theta(g) = \mathcal{O}(g) \cap \Omega(g)
  ```

### Aggregate method (brute force)

Analyzujeme celou sekvenci operacÃ­ najednou. NepouÅ¾Ã­vÃ¡me Å¾Ã¡dnÃ© chytristiky ani fÃ­gle.

**ZÃ¡sobnÃ­k (brute force)**

- **VÄ›ta**\
  Pokud zaÄneme s prÃ¡zdnÃ½m zÃ¡sobnÃ­kem, pak libovolnÃ¡ posloupnost $n$ operacÃ­ `Push`, `Pop` a `Multi-Pop` zabere $\mathcal{O}(n)$ Äasu.
- **DÅ¯kaz**
  - KaÅ¾dÃ½ prvek je `Pop`nut nejvÃ½Å¡e jednou pro kaÅ¾dÃ½ jeho `Push`.
  - V posloupnosti je $\le n$ `Push`Å¯.
  - V posloupnosti je $\le n$ `Pop`Å¯ (vÄetnÄ› tÄ›ch v `Multi-Pop`u).
  - CelÃ¡ posloupnost mÃ¡ tak nejvÃ½Å¡e sloÅ¾itost $2n$.

### Accounting method (banker's method)

PouÅ¾Ã­vÃ¡ fÃ­gl, kdy velkÃ© mnoÅ¾stvÃ­ _levnÃ½ch_ operacÃ­ "pÅ™edplatÃ­" jednu _drahou_ operaci. VyuÅ¾Ã­vÃ¡ metaforu bankovnÃ­ho ÃºÄtu.

- KaÅ¾dÃ© operaci pÅ™iÅ™adÃ­me fiktivnÃ­ _kreditovou_ cenu.
- PÅ™i realizaci operace zaplatÃ­me _skuteÄnou_ cenu naspoÅ™enÃ½mi kredity.
- PoÄÃ¡teÄnÃ­ stav je 0 kreditÅ¯.

Pro kaÅ¾dou operaci v posloupnosti:

- Pokud je _skuteÄnÃ¡_ cena niÅ¾Å¡Ã­ neÅ¾ _kreditovÃ¡_, tak zaplatÃ­me skuteÄnou cenu a pÅ™ebÃ½vajÃ­cÃ­ kredity uspoÅ™Ã­me na _ÃºÄtu_.
- Pokud je _skuteÄnÃ¡_ cena vyÅ¡Å¡Ã­ neÅ¾ _kreditovÃ¡_, tak zaplatÃ­me skuteÄnou cenu a pÅ™Ã­padnÃ½ nedostatek kreditÅ¯ doplatÃ­me z Ãºspor na _ÃºÄtu_.

**â— IMPORTANT**\
Pokud je po celou dobu provÃ¡dÄ›nÃ­ operacÃ­ stav ÃºÄtu **nezÃ¡pornÃ½**, pak je _skuteÄnÃ¡_ sloÅ¾itost celÃ© posloupnosti operacÃ­ menÅ¡Ã­ nebo rovna souÄtu _kreditovÃ½ch_ cen operacÃ­.

**âš ï¸ WARNING**\
Pokud stav ÃºÄtu **kdykoliv bÄ›hem posloupnosti** klesne pod nulu, pak jsou kreditovÃ© ceny nastaveny **Å¡patnÄ›**!

**ğŸ’¡ TIP**\
Tato metoda se dÃ¡ upravit tak, Å¾e kredity nÃ¡leÅ¾Ã­ individuÃ¡lnÃ­m objektÅ¯m ve struktuÅ™e mÃ­sto struktury jako celku. Cena operace se pak platÃ­ z kreditÅ¯ objektÅ¯, nad kterÃ½m operace probÃ­hÃ¡.

**ZÃ¡sobnÃ­k (kredity)**

| Operace         |
| --------------- |
| SkuteÄnÃ¡ cena   |
| KreditovÃ¡ cena  |
| `Push`          |
| 1               |
| 2               |
| `Pop`           |
| 1               |
| 0               |
| `Multi-Pop`     |
| stem:[\min(k,\  |
| S\              |
| )]              |
| 0               |

- **Invariant**\
  PoÄet kreditÅ¯ na ÃºÄtu je rovnÃ½ poÄtu prvkÅ¯ na zÃ¡sobnÃ­ku.
- **DÅ¯kaz**
  - Invariant platÃ­ pro prÃ¡dnÃ½ zÃ¡sobnÃ­k.
  - S `Push` operacÃ­ se na ÃºÄet pÅ™ipÃ­Å¡e prÃ¡vÄ› 1 kredit. (ÄŒÃ­mÅ¾ se pÅ™edplatÃ­ `Pop` nebo `Multi-Pop`.)
  - `Pop` a `Multi-Pop` operace spotÅ™ebujÃ­ prÃ¡vÄ› 1 kredit z ÃºÄtu.
  - Tedy stav ÃºÄtu nikdy neklesne pod 0.
  - Tedy sloÅ¾itost posloupnosti je nejvÃ½Å¡e souÄet kreditovÃ½ch cen, tedy $2n$.

### Potential method (physicist's method)

Hraje si s pÅ™edstavou toho, Å¾e struktura je fyzikÃ¡lnÃ­ systÃ©m s nÄ›jakou energetickou hladinou -- potenciÃ¡lem. VÃ½hodou tÃ©to metody je, Å¾e staÄÃ­ zvolit _jednu_ funkci, kterÃ¡ splÅˆuje danÃ© podmÃ­nky. NevÃ½hodou je, Å¾e takovou funkci najÃ­t je tÄ›Å¾kÃ©. ÄŒlovÄ›k zkrÃ¡tka buÄ dostane nÃ¡pad nebo ne.

- **PotenciÃ¡lovÃ¡ funkce**\
  Funkce $\Phi$, kterÃ¡ pÅ™iÅ™adÃ­ danÃ© struktuÅ™e $S$ hodnotu. PlatÃ­, Å¾e:

  ```math
  \begin{align*}
  \Phi(S_0) &= 0 \text{, kde } S_0 \text{ je poÄÃ¡teÄnÃ­ stav} \\
  \Phi(S_i) &\ge 0 \text{ pro kaÅ¾dou strukturu } S_i
  \end{align*}
  ```

- **AmortizovanÃ¡ cena**\
  Pokud $c_i$ je _skuteÄnÃ¡_ cena operace, pak pro amortizovanou cenu $\hat{c_i}$ platÃ­:

  ```math
  \hat{c_i} = c_i + \Phi(S_i) - \Phi(S_{i-1})
  ```

- **PotenciÃ¡lovÃ¡ vÄ›ta**\
  PoÄÃ­naje poÄÃ¡teÄnÃ­m stavem $S_0$, celkovÃ¡ _skuteÄnÃ¡_ cena posloupnosti $n$ operacÃ­ je nejvÃ½Å¡e souÄet jejich amortizovanÃ½ch cen.
- **DÅ¯kaz**

  ```math
  \begin{align*}
  \sum_{i=1}^n \hat{c_i} &= \sum_{i=1}^n (c_i + \Phi(S_i) - \Phi(S_{i-1})) \\
  &= \sum_{i=1}^n c_i + \Phi(S_n) - \Phi(S_0) \\
  &\geq \sum_{i=1}^n c_i \quad\tiny\blacksquare
  \end{align*}
  ```

**ZÃ¡sobnÃ­k (potenciÃ¡lovÃ¡ vÄ›ta)**

$\Phi(S) = |S|$ (poÄet prvkÅ¯ na zÃ¡sobnÃ­ku)

| Operace                   |
| ------------------------- |
| SkuteÄnÃ¡ cena             |
| AmortizovanÃ¡ cena         |
| `Push`                    |
| 1                         |
| stem:[\hat{c_i} = 1 + (\  |
| S\                        |
| + 1) - \                  |
| S\                        |
| = 2]                      |
| `Pop`                     |
| 1                         |
| stem:[\hat{c_i} = 1 + \   |
| S\                        |
| - (\                      |
| S\                        |
| + 1) = 0]                 |
| `Multi-Pop`               |
| stem:[\min(k,\            |
| S\                        |
| )]                        |
|                           |

```math
\hat{c_i} =
\begin{cases}
k + (\|S\| - k) - \|S\| = 0 & \text{pokud } \|S\| > k \\
\|S\| + (\|S\| - \|S\|) - \|S\| = 0 & \text{pokud } \|S\| \le k

\end{cases}
```

- **VÄ›ta**\
  PoÄÃ­naje prÃ¡zdnÃ½m zÃ¡sobnÃ­kem, libovolnÃ¡ sekvence operacÃ­ zabere $\mathcal{O}(n)$ Äasu.
- **DÅ¯kaz (pÅ™Ã­pad `Push`)**
  - SkuteÄnÃ¡ cena je $c_i = 1$.
  - AmortizovanÃ¡ cena je $\hat{c_i} = c_i + \Phi(S_i) - \Phi(S_{i-1}) = 1 + (|S| + 1) - |S| = 2$.
- **DÅ¯kaz (pÅ™Ã­pad `Pop`)**
  - SkuteÄnÃ¡ cena je $c_i = 1$.
  - AmortizovanÃ¡ cena je $\hat{c_i} = c_i + \Phi(S_i) - \Phi(S_{i-1}) = 1 - 1 = 0$.
- **DÅ¯kaz (pÅ™Ã­pad `Multi-Pop`)**
  - SkuteÄnÃ¡ cena je $c_i = k$.
  - AmortizovanÃ¡ cena je $\hat{c_i} = c_i + \Phi(S_i) - \Phi(S_{i-1}) = k - k = 0$.
- **DÅ¯kaz (zÃ¡vÄ›r)**
  - AmortizovanÃ¡ cena vÅ¡ech operacÃ­ je $\hat{c_i} \le 2$.
  - SouÄet amortizovanÃ½ch cen posloupnosti $n$ operacÃ­ je pak $\sum_{i=1}^n \hat{c_i} \le 2n$.
  - Z potenciÃ¡lnovÃ© vÄ›ty plyne, Å¾e skuteÄnÃ¡ cena posloupnosti je $\le 2n$.

---

**SlavnÃ© potenciÃ¡lovÃ© funkce**

- **Fibonnacciho halda**

  ```math
  \Phi(H) = 2 \cdot \text{trees}(H) - 2 \cdot \text{marks}(H)
  ```

- **Splay trees**\
  BinÃ¡rnÃ­ vyhledÃ¡vacÃ­ stromy, kde poslednÃ­ pÅ™Ã­danÃ© prvky jsou pÅ™Ã­stupnÃ© rychleji. [(zdroj)](https://en.wikipedia.org/wiki/Splay_tree)

  ```math
  \Phi(T) = \sum_{x \in T} \lfloor \log_2 \text{size}(x) \rfloor
  ```

- **Move-to-front**\
  Transformace dat pouÅ¾Ã­vanÃ¡ pÅ™i kompresi dat. [(zdroj)](https://en.wikipedia.org/wiki/Move-to-front_transform)

  ```math
  \Phi(L) = 2 \cdot \text{inversions}(L, L^*)
  ```

- **Preflow-push (push-relabel)**

  ```math
  \Phi(f) = \sum_{v \,:\, \text{excess}(v) > 0} \text{height}(v)
  ```

- **Red-black trees**

  ```math
  \Phi(T) = \sum_{x \in T} w(x)

  \\

  w(x) = \begin{cases}
  0 & \text{pokud } x \text{ je ÄervenÃ½} \\
  1 & \text{pokud } x \text{ je ÄernÃ½ a nemÃ¡ Å¾Ã¡dnÃ© ÄervenÃ© potomky} \\
  0 & \text{pokud } x \text{ je ÄernÃ½ a mÃ¡ jednoho ÄervenÃ©ho potomka} \\
  2 & \text{pokud } x \text{ je ÄernÃ½ a mÃ¡ dva ÄervenÃ© potomky}
  \end{cases}
  ```

## VyhledÃ¡vÃ¡nÃ­ Å™etÄ›zcÅ¯ (string matching)

_String matching_ oznaÄuje rodinu problÃ©mÅ¯ obsahujÃ­cÃ­ tÅ™eba:

- NalezenÃ­ prvnÃ­ho pÅ™esnÃ©ho vÃ½skytu podÅ™etÄ›zce (_patternu_) v Å™etÄ›zci (_stringu_ / _textu_).
- NalezenÃ­ vÅ¡ech vÃ½skytÅ¯ podÅ™etÄ›zce v Å™etÄ›zci.
- VÃ½poÄet vzdÃ¡lenosti dvou Å™etÄ›zcÅ¯.
- HledÃ¡nÃ­ opakujÃ­cÃ­ch se podÅ™etÄ›zcÅ¯.

VÄ›tÅ¡inou je Å™etÄ›zec polem znakÅ¯ z koneÄnÃ© abecedy $\Sigma$. String matching algoritmy se ale dajÃ­ pouÅ¾Ã­t na ledacos.

Vzorek $P$ se vyskytuje v textu $T$ s posunem $s$, pokud $0 \le s \le n - m$ a zÃ¡roveÅˆ $T\lbrack (s+1) .. (s + m) \rbrack = P$. Pro nalezenÃ­ platnÃ½ch posunÅ¯ lze pouÅ¾Ã­t Å™adu algoritmÅ¯, kterÃ© se liÅ¡Ã­ sloÅ¾itostÃ­ pÅ™edzpracovÃ¡nÃ­ i samotnÃ©ho vyhledÃ¡vÃ¡nÃ­: [iv003-strings](#iv003-strings)

| Algoritmus                          |
| ----------------------------------- |
| Preprocessing                       |
| Searching                           |
| Brute force / naivnÃ­                |
| $0$                                 |
| $\mathcal{O}((n - m + 1) \cdot m)$  |
| Karp-Rabin                          |
| $\Theta(m)$                         |
| $\mathcal{O}((n - m + 1) \cdot m)$  |
| finite automata                     |
| stem:[\Theta(m \cdot \              |
| \Sigma\                             |
| )]                                  |
| $\Theta(n)$                         |
| Knuth-Morris-Pratt                  |
| $\Theta(m)$                         |
| $\Theta(m)$                         |
| Boyer-Moore                         |
| stem:[\Theta(m + \                  |
| \Sigma\                             |
| )]                                  |
| $\mathcal{O}((n - m  + 1) \cdot m)$ |

- $T$ nebo $T\lbrack 1..n \rbrack$ -- text.
- $P$ nebo $P\lbrack 1..m \rbrack$ -- pattern.
- $n$ -- dÃ©lka textu $T$.
- $m$ -- dÃ©lka vzorku / podÅ™etÄ›zce / patternu $P$.
- $\Sigma$ -- koneÄnÃ¡ abeceda, ze kterÃ© je sloÅ¾en text i pattern.

### Brute force / naivnÃ­

ProchÃ¡zÃ­ vÅ¡echny pozice v textu a porovnÃ¡vÃ¡ je s patternem. Pokud se neshodujÃ­, posune se o jedno pole dopÅ™edu.

Pokud se text neshoduje uÅ¾ v prvnÃ­m znaku, je sloÅ¾itost lineÃ¡rnÃ­. AvÅ¡ak v nejhorÅ¡Ã­m pÅ™Ã­padÄ›, kdy se pattern shoduje s textem aÅ¾ na poslednÃ­ znak, je sloÅ¾itost aÅ¾ kvadratickÃ¡.

```csharp
int Naive(string text, string pattern)
{
    int n = text.Length;
    int m = pattern.Length;
    for (int i = 0; i < n - m + 1; i++)
    {
        // NB: Substring creates a new string but let's not worry about that.
        if (text.Substring(i, m) == pattern)
        {
            return i;
        }
    }
    return -1;
}
```

- **SloÅ¾itost**\
  Cyklus je proveden $n-m+1$-krÃ¡t. V kaÅ¾dÃ© iteraci se provede pÅ™inejhorÅ¡Ã­m aÅ¾ $m$ porovnÃ¡nÃ­. (ZanedbÃ¡vÃ¡me sloÅ¾itost `Substring`, kterÃ¡ v C# dÄ›lÃ¡ kopii.)

### Karp-Rabin

PouÅ¾Ã­vÃ¡ hashovÃ¡nÃ­. VytvoÅ™Ã­ hash z patternu a hashuje i vÅ¡echny podÅ™etÄ›zce dÃ©lky $m$ v textu. Nejprve eliminuje vÅ¡echny pozice, kde se hashe neshodujÃ­. Pokud se shodujÃ­, porovnÃ¡ je znak po znaku.

```csharp
int KarpRabin(string text, string pattern)
{
    int n = text.Length;
    int m = pattern.Length;
    int patternHash = pattern.GetHash();
    for (int i = 0; i < n - m + 1; i++)
    {
        // NB: Assume that `GetHash` is a rolling hash, even though in .NET it's not.
        if (text.Substring(i, m).GetHash() == patternHash)
        {
            if (text.Substring(i, m) == pattern)
            {
                return i;
            }
        }
    }
    return -1;
}
```

- **HashovÃ¡nÃ­**\
  Trik spoÄÃ­vÃ¡ v pouÅ¾itÃ­ _rolling_ hashovacÃ­ho algoritmu. Ten je schopnÃ½ pÅ™i vÃ½poÄtu hashe pro $T\lbrack s .. (s + m) \rbrack$ pouÅ¾Ã­t hash $T\lbrack s .. (s + m - 1) \rbrack$ s vyuÅ¾itÃ­m pouze jednoho dalÅ¡Ã­ho znaku $T\lbrack s + m \rbrack$. PÅ™epoÄet hashe je tedy $\mathcal{O}(1)$.

  Jeden takovÃ½ algoritmus lze zÃ­skat pouÅ¾itÃ­m Hornerova schÃ©matu pro evaluaci polynomu. [horner](#horner) PÅ™edpoklÃ¡dejme, Å¾e $\Sigma = \{0, 1, ..., 9\}$ (velikost mÅ¯Å¾e bÃ½t libovolnÃ¡), pak pro hash $h$ platÃ­

  ```math
  \begin{align*}
  h &= P[1] + 10 \cdot P[2] + 10^2 \cdot P[3] + ... + 10^{m-1} \cdot P[m] \\
    &= P[1] + 10 \cdot (P[2] + 10 \cdot (P[3] + ... + 10 \cdot P[m] ... ))
  \end{align*}
  ```

  Pokud jsou hashe pÅ™Ã­liÅ¡ velkÃ©, lze navÃ­c pouÅ¾Ã­t modulo $q$, kde $10 \cdot q \approx \text{machine word}$.

- **SloÅ¾itost**\
  PÅ™edzpracovÃ¡nÃ­ zahrnuje vÃ½poÄet $T \lbrack 1..m \rbrack$ v $\Theta(m)$.

  SloÅ¾itost vÃ½poÄtu je v nejhorÅ¡Ã­m pÅ™Ã­padÄ› $\mathcal{O}((n - m + 1) \cdot m)$, jelikoÅ¾ je potÅ™eba porovnat vÅ¡echny podÅ™etÄ›zce dÃ©lky $m$ s patternem.

  Tento algoritmus se hodÃ­ pouÅ¾Ã­t, pokud hledÃ¡me v textu celÃ© vÄ›ty, protoÅ¾e neoÄekÃ¡vÃ¡me velkÃ© mnoÅ¾stvÃ­ "faleÅ¡nÃ½ch" shod, kterÃ© majÃ­ stejnÃ½ hash jako $P$. V tomto pÅ™Ã­padÄ› je prÅ¯mÄ›rnÃ¡ sloÅ¾itost $\mathcal{O}(n)$.

### KoneÄnÃ© automaty

SloÅ¾itost naivnÃ­ho algortmu lze vylepÅ¡it pouÅ¾itÃ­m koneÄnÃ©ho automatu.

MÄ›jmÄ› DFA $A = (\{0, ..., m\}, \Sigma, \delta, \{0\}, \{m\})$, kde pÅ™echodobou funkci definujeme jako:

```math
\delta(q, x) = \text{nejvÄ›tÅ¡Ã­ } k \text{ takovÃ©, Å¾e } P[1..k] \text{ je \textbf{prefix} a zÃ¡roveÅˆ \textbf{suffix} } P[1..q] . x
```

JinÃ½mi slovy, $\delta$ vracÃ­ delkÅ¯ nejdelÅ¡Ã­ho moÅ¾nÃ©ho zaÄÃ¡tku $P$, kterÃ½ se nachÃ¡zÃ­ na danÃ©m mÃ­stÄ› (stavu $q$) v Å™etÄ›zci $T$.

Prakticky by pÅ™Ã­prava pÅ™echodovÃ© funkce mohla vypadat takto:

```csharp
int[,] CreateAutomaton(string pattern)
{
    int m = pattern.Length;
    // NB: Assumes that the alphabet is ASCII.
    int[,] automaton = new int[m + 1, 256];
    for (int q = 0; q <= m; q++)
    {
        for (int c = 0; c < 256; c++)
        {
            int k = Math.Min(m + 1, q + 2);
            do
            {
                k--;
            }
            while (!pattern.Substring(0, q).Equals(pattern.Substring(0, k) + (char)c));
            automaton[q, c] = k;
        }
    }
    return automaton;
}
```

VyhledÃ¡vÃ¡nÃ­ v textu pak bude vypadat takto:

```csharp
int FiniteAutomaton(string text, string pattern)
{
    int n = text.Length;
    int m = pattern.Length;
    int[,] automaton = CreateAutomaton(pattern);
    int q = 0;
    for (int i = 0; i < n; i++)
    {
        q = automaton[q, text[i]];
        if (q == m)
        {
            return i - m + 1;
        }
    }
    return -1;
}
```

Tato metoda Å¡etÅ™Ã­ Äas, pokud se pattern v nÄ›kterÃ½ch mÃ­stech opakuje. MÄ›jmÄ› napÅ™Ã­klad pattern `abcDabcE` a text `abcDabcDabcE`. Tato metoda nemusÃ­ zaÄÃ­nat porovnÃ¡vat pattern od zaÄÃ¡tku po pÅ™eÄtenÃ­ druhÃ©ho `D`, ale zaÄne od $P \lbrack 5 \rbrack$ (vÄetnÄ›), protoÅ¾e _vÃ­_, Å¾e pÅ™edchozÃ­ ÄÃ¡st patternu se jiÅ¾ vyskytla v textu.

JinÃ½mi slovy na indexu druhÃ©ho `D` je `abcD` nejdelÅ¡Ã­ prefix $P$, kterÃ½ je zÃ¡roveÅˆ suffixem uÅ¾ naÄtenÃ©ho Å™etÄ›zce.

- **SloÅ¾itost**\
  VytvoÅ™enÃ­ automatu vyÅ¾aduje $\Theta(m^3 \cdot |\Sigma|)$ Äasu, dÃ¡ se vÅ¡ak provÃ©st efektivnÄ›ji neÅ¾ v `CreateAutomaton` a to v Äase $\Theta(m \cdot |\Sigma|)$.

  SloÅ¾itost hledÃ¡nÃ­ je pak v $\Theta(n)$. [iv003-strings](#iv003-strings)

### Knuth-Morris-Pratt (KMP)

KMP pÅ™edstavuje efektivnÄ›jÅ¡Ã­ vyuÅ¾itÃ­ idei z metody koneÄnÃ©ho automatu:

- KaÅ¾dÃ½ stav $q$ je oznaÄen pÃ­smenem z patternu. VÃ½jimkou je poÄÃ¡teÄnÃ­ stav $S$ a koncovÃ½ stav $F$.
- KaÅ¾dÃ½ stav mÃ¡ hranu `success`, kterÃ¡ popisuje sekvenci znakÅ¯ z patternu, a `failure` hranu, kterÃ¡ mÃ­Å™Ã­ do nÄ›kterÃ©ho z pÅ™edchozÃ­ch stavÅ¯ -- takovÃ©ho, Å¾e uÅ¾ naÄtenÃ© znaky jsou nejvÄ›tÅ¡Ã­ moÅ¾nÃ½ prefix patternu.

V reÃ¡lnÃ© implementaci nejsou `success` hrany potÅ™eba; potÅ™ebujeme jen vÄ›dÄ›t, kam skoÄit v pÅ™Ã­padÄ› neÃºspÄ›chu.

```csharp
/// <summary>
/// Computes the longest proper prefix of P[0..i]
/// that is also a suffix of P[0..i].
/// </summary>
int[] ComputeFailure(string pattern)
{
    int m = pattern.Length;
    int[] fail = new int[m];
    int j = 0;
    for (int i = 1; i < m; i++)
    {
        while (j >= 0 && pattern[j] != pattern[i])
        {
            j = fail[j];
        }

        // If comparison at i fails,
        // return to j as the new starting point.
        fail[i] = j;

        j++;
    }
    return fail;
}

int KnuthMorrisPratt(string text, string pattern)
{
    int[] fail = ComputeFailure(pattern);
    int n = text.Length;
    int m = pattern.Length;
    // NB: I index from 0 here. Although I use 1..n in the text.
    int i = 0;
    int j = 0;
    for (int i = 0; i < n; i++)
    {
        while (j >= 0 && text[i] != pattern[j])
        {
            /*
                There can be at most n-1 failed comparisons
                since the number of times we decrease j cannot
                exceed the number of times we increment i.
            */
            j = fail[j];
        }

        j++;
        if (j == m)
        {
            return i - m;
        }
    }
    return -1;
}
```

**âš ï¸ WARNING**\
Nejsem si jistÃ½, Å¾e ty indexy v kÃ³du vÃ½Å¡e mÃ¡m dobÅ™e.

**ğŸ“Œ NOTE**\
"In other words we can amortize character mismatches against earlier character matches." [iv003-strings](#iv003-strings)

- **SloÅ¾itost**\
  AmortizacÃ­ neÃºspÄ›Å¡nÃ½ch porovnÃ¡nÃ­ vÅ¯Äi ÃºspÄ›Å¡nÃ½m zÃ­skÃ¡me $\mathcal{O}(m)$ pro `ComputeFailure` a $\mathcal{O}(n)$ pro `KnuthMorrisPratt`.

## Zdroje

- [[[iv003, 1]]] [IV003 Algoritmy a datovÃ© struktury II (jaro 2021)](https://is.muni.cz/auth/el/fi/jaro2021/IV003/)
- [[[iv003-strings,2]]] https://is.muni.cz/auth/el/fi/jaro2021/IV003/um/slides/stringmatching.pdf
- [[[rabin-karp-wiki,3]]] https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm
- [[[horner,4]]] https://en.wikipedia.org/wiki/Horner%27s_method
- [[[backtracking,5]]] https://betterprogramming.pub/the-technical-interview-guide-to-backtracking-e1a03ca4abad
