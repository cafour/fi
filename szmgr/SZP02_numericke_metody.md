---
title: "Numerick√© metody"
description: "TODO"
---

<dl><dt><strong>üìå NOTE</strong></dt><dd>

Iterativn√≠ metody pro ≈ôe≈°en√≠ neline√°rn√≠ch rovnic (Newtonova metoda a jej√≠ modifikace). P≈ô√≠m√© metody pro ≈ôe≈°en√≠ syst√©mu line√°rn√≠ch rovnic (Gaussova eliminace, Jacobi, Gauss-Seidel, relaxaƒçn√≠ metody). Numerick√° diferenciace, diferenciaƒçn√≠ sch√©mata.

_MA018_

</dd></dl>

- **Numerick√° anal√Ωza / numerical analysis**\
  Oblast matematiky / informatiky zab√Ωvaj√≠c√≠ se tvorbou numerick√Ωch metod a algoritm≈Ø, kter√© ≈ôe≈°√≠ probl√©my matematick√© anal√Ωzy (nap≈ô. derivace, integr√°ly a podobn√Ω symbolick√Ω balast) pomoc√≠ numerick√© aproximace. [numerical-analysis](#numerical-analysis)

  Je v√Ωhodn√° v situac√≠ch, kdy probl√©m nelze ≈ôe≈°it analyticky nebo je to p≈ô√≠li≈° slo≈æit√© a nen√≠ to (v√Ωpoƒçetn√≠) ƒças.

- **Notace chyb**

  - $x$ je p≈ôesn√° hodnota,
  - $\tilde{x}$ je aproximace $x$,
  - $x - \tilde{x}$ je absolutn√≠ chyba $\tilde{x}$,
  - $\lvert x - \tilde{x} \rvert \leq \alpha$ je odhad absolutn√≠ chyby,
  - $\frac{x - \tilde{x}}{x}$ je relativn√≠ chyba,
  - $\left\lvert \frac{x - \tilde{x}}{x} \right\rvert \leq \alpha$ je odhad relativn√≠ chyby.

- **Numerick√° stabilita**\
  Schopnost numerick√© metody zpracovat chyby vstupn√≠ch dat a v√Ωpoƒçetn√≠ch operac√≠.

  Desetinn√° ƒç√≠sla jsou v poƒç√≠taƒç√≠ch nevyhnutelnƒõ reprezentov√°na nep≈ôesnƒõ. Numericky stabiln√≠ metody jsou takov√©, kter√© tyto nep≈ôesnosti **nezhor≈°uj√≠**. [numerical-stability](#numerical-stability)

- **≈ò√°d metody / order of accuracy / order of approximation**\
  Hodnota reprezentuj√≠c√≠, jak rychle metoda konverguje k v√Ωsledku, resp. jak p≈ôesn√Ω je jej√≠ odhad.

  Numerick√° metoda obvykle konverguje sni≈æov√°n√≠m nƒõjak√©ho _kroku_ $h$. Pokud ho lze zvolit libovolnƒõ mal√Ω, a lze-li prohl√°sit, ≈æe pro chybu aproximace $E$ plat√≠: [rate](#rate) [numericka-metoda](#numericka-metoda) [order-question](#order-question)

  ```math
  \begin{aligned}

  E(h) &\leq C \cdot h^p \\
  E(h) &\in \mathcal{O}(h^p)

  \end{aligned}
  ```

  kde $C$ je konstanta. Pak $p$ je ≈ô√°d metody.

## Iterativn√≠ metody pro ≈ôe≈°en√≠ neline√°rn√≠ch rovnic

- **Root-finding problem**\
  Probl√©m nalezen√≠ _ko≈ôen≈Ø_ (root) funkce $f$. T.j. takov√Ωch parametr≈Ø $x, ...$, kde funkce vrac√≠ 0: [root-finding](#root-finding)

  ```math
  f(x, ...) = 0
  ```

- **Iterative methods for root-finding problem**\
  Metody pro ≈ôe≈°en√≠ root-finding problemu, kter√© vyu≈æ√≠vaj√≠ iterativn√≠ho p≈ô√≠stupu. Tedy opakuj√≠ nƒõjak√Ω v√Ωpoƒçet a zp≈ôes≈àuj√≠ sv≈Øj odhad, dokud nedos√°hnou po≈æadovan√© p≈ôesnosti. [ma018](#ma018) [root-finding](#root-finding)
- **≈ò√°d metody / rate of convergence**\
  Hodnota reprezentuj√≠c√≠, jak rychle metoda konverguje k v√Ωsledku. [rate](#rate)
- **Prost√° iteraƒçn√≠ metoda / metoda pevn√©ho bodu / fixed-point iteration**\
  Pou≈æ√≠v√° se pro rovnice typu $x = g(x)$.

  1. Zvol√≠me poƒç√°teƒçn√≠ odhad $x_0$.
  2. Opakujeme $x_{n+1} = g(x_n)$ dokud $\lvert x_{n+1} - x_n \rvert \leq \alpha$ (kde $\alpha$ je po≈æadovan√° p≈ôesnost).

     ![width=400](img/szp02_fixed_point_method.png)

- **Newtonova metoda / metoda teƒçen**\
  Pou≈æ√≠v√° k odhadu ko≈ôene funkce $f$ jej√≠ teƒçnu v bodƒõ $x_n$. Iteraƒçn√≠ funkce je:

  ```math
  g(x_{k+1}) = x_k - \frac{f(x_k)}{f'(x_k)}
  ```

  1. Zvol√≠me poƒç√°teƒçn√≠ odhad $x_0$.
  2. Dal≈°√≠ odhad je $x_{n+1} = g(x_n)$, tedy pr≈Øseƒç√≠k teƒçny fukce $f$ v bodƒõ $x_n$ s osou $x$.
  3. Opakujeme 2. dokud nedos√°hneme po≈æadovan√© p≈ôesnosti odhadu.

     ![width=400](./img/szp02_newton_method.png)

- **Metoda seƒçen / secant method**\
  Pou≈æ√≠v√° k odhadu ko≈ôene funkce $f$ seƒçny, resp. _finite difference_, kter√© aproximuj√≠ derivaci funkce $f$. D√≠ky tomu nen√≠ pot≈ôeba zn√°t derivaci funkce $f$. Iteraƒçn√≠ funkce je:

  ```math
  g(x_{k+1}) = x_k - \frac{f(x_k)(x_k - x_{k-1})}{f(x_k) - f(x_{k-1})}
  ```

  ![width=400](./img/szp02_secant_method.png)

- **Metoda regula falsi**\
  Je _bracketing_ metoda, tedy metoda, kter√° vyu≈æ√≠v√° intervalu, ve kter√©m se nach√°z√≠ ko≈ôen. Nemus√≠ se pou≈æ√≠t iterativnƒõ, ale v iterativn√≠ podobƒõ tento interval postupnƒõ zmen≈°uje. [regula-falsi](#regula-falsi)

  ```math
  x_{k+1} = x_k - \frac{x_k - x_s}{f(x_k) - f(x_s)} f(x_k)
  ```

  kde $s$ je nejvƒõt≈°√≠ index takov√Ω, ≈æe $f(x_k)f(x_s) &lt; 0$.

## P≈ô√≠m√© metody pro ≈ôe≈°en√≠ syst√©mu line√°rn√≠ch rovnic

### Gaussova eliminace

Syst√©m rovnice je p≈ôeps√°n do matice. Gaussova eliminace je posloupnost operac√≠, jejich≈æ c√≠lem je p≈ôev√©st matici do horn√≠ troj√∫heln√≠kov√© matice (_row echelon form_). [gauss-elimination](#gauss-elimination) Povoleny jsou n√°sleduj√≠c√≠ operace:

- v√Ωmƒõna dvou ≈ô√°dk≈Ø,
- vyn√°soben√≠ ≈ô√°dku nenulovou konstantou,
- p≈ôiƒçten√≠ n√°sobku jednoho ≈ô√°dku k jin√©mu.

### Jacobiho iteraƒçn√≠ metoda

Iterativn√≠ algoritmus pro ≈ôe≈°en√≠ soustavy line√°rn√≠ch rovnic. Rozdƒõluje vstupn√≠ matici line√°rn√≠ch rovnic na matici diagon√°l $D$, doln√≠ troj√∫heln√≠kovou matici $L$ a horn√≠ troj√∫heln√≠kovou matici $U$. [jacobi-method](#jacobi-method)

Nech≈• $A\mathbf{x} = \mathbf{b}$ je syst√©m $n$ line√°rn√≠ch rovnic. Tedy:

```math
A = \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}, \qquad

\mathbf{x} = \begin{bmatrix}
    x_{1} \\ x_2 \\ \vdots \\ x_n
\end{bmatrix} , \qquad

\mathbf{b} = \begin{bmatrix}
    b_{1} \\ b_2 \\ \vdots \\ b_n
\end{bmatrix}.
```

Algoritmus vypad√° takto:

1. Zvol√≠me poƒç√°teƒçn√≠ odhad $\mathbf{x}^{(0)}$, nejƒçastƒõji $\vec{0}$.
2. Nov√Ω odhad z√≠sk√°me ze vztahu:

   ```math
   \mathbf{x}^{(k+1)} = D^{-1}(\mathbf{b} - (L + U)\mathbf{x}^{(k)})
   ```

Jeliko≈æ $L + U = A - D$, d√° to zapsat i jako:

```math
\mathbf{x}^{(k+1)} = D^{-1}\mathbf{b} + (I - D^{-1} A) \mathbf{x}^{(k)}
```

- **Spektr√°ln√≠ polomƒõr**\
  Spektr√°ln√≠ polomƒõr $\rho$ matice $A$ je nejvƒõt≈°√≠ absolutn√≠ hodnota vlastn√≠ho ƒç√≠sla matice $A$.

  ```math
  \rho(A) = \max_{i=1,\ldots,n} |\lambda_i|
  ```

- **(≈ò√°dkov√°) diagon√°ln√≠ dominance**\
  Matice $A$ je diagon√°lnƒõ dominantn√≠, pokud plat√≠:

  ```math
  |a_{ii}| > \sum_{j=1, j \neq i}^n |a_{ij}|
  ```

  Tedy absolutn√≠ hodnota prvku na diagon√°le je vƒõt≈°√≠ ne≈æ souƒçet absolutn√≠ch hodnot v≈°ech ostatn√≠ch prvk≈Ø v ≈ô√°dku.

  - _Striktn√≠_: nerovnost je ostr√° ($>$).
  - _Slab√°_: nerovnost je neost√° ($\ge$).

  Analogicky se definuje sloupcov√° diagon√°ln√≠ dominance.

- **Konvergence Jacobiho metody**\
  Jacobiho metoda konveguje pokud v≈°echny n√°sleduj√≠c√≠ podm√≠nky:

  1. Nech≈• $T_j = I - D^{-1} A$ je matice iterace Jacobiho metody. Pak Jacobiho metoda konverguje, pokud:

     ```math
     \rho(T_j) < 1
     ```

  2. Jacobiho metoda konverguje pro libovoln√Ω poƒç√°teƒçn√≠ odhad $\mathbf{x}^{(0)}$, pokud $A$ je diagon√°lnƒõ dominantn√≠ (sloupcovƒõ nebo ≈ô√°dkovƒõ).

### Gaussova-Seidelova iteraƒçn√≠ metoda

Iterativn√≠ metoda pro ≈ôe≈°en√≠ soustavy line√°rn√≠ch rovnic. Dƒõl√≠ vstupn√≠ matici na spodn√≠ troj√∫heln√≠kovou matici $L_*$ (vƒçetnƒõ diagon√°ly, tedy $L_* = D + L$) a striktnƒõ horn√≠ troj√∫heln√≠kovou matici $U$ (diagon√°la je nulov√°). Algoritmus vypad√° takto: [gauss-seidel](#gauss-seidel)

1. Zvol√≠me poƒç√°teƒçn√≠ odhad $\mathbf{x}^{(0)}$.
2. Nov√Ω odhad z√≠sk√°me ze vztahu:

   ```math
   \mathbf{x}^{(k+1)} = L_*^{-1}(\mathbf{b} - U\mathbf{x}^{(k)}).
   ```

Alternativnƒõ:

```math
\begin{aligned}

T_{gs} &= (D + L)^{-1} U = L_*^{-1} U \\
\mathbf{x}^{(k+1)} &= T_{gs} \mathbf{x}^{(k)} + g,\quad g = L_*^{-1} \mathbf{b}

\end{aligned}
```

- **Konvergence Gaussovy-Seidelovy metody**\
  Analogicky jako u Jacobiho metody, ale m√≠sto matice $T_j$ se pou≈æije matice $T_{gs} = (D + L)^{-1} U$.

### Relaxaƒçn√≠ iterativn√≠ metody

Modifikace Gauss-Seidelovy metody. Vyu≈æ√≠v√° parametr $\omega$, kter√Ω urƒçuje, jak moc se m√° nov√Ω odhad li≈°it od p≈ôedchoz√≠ho. Vztah pro dal≈°√≠ iteraci se mƒõn√≠ na: [relaxation-method](#relaxation-method)

```math
\begin{align*}
    \mathbf{x}^{(k+1)} &= (D - \omega L)^{-1} [(1-\omega)D + \omega U]\mathbf{x}^{(k)} + \omega(D - \omega L)^{-1}\mathbf{b} \\
    T_\omega &= (D - \omega L)^{-1} [(1-\omega)D + \omega U]
\end{align*}
```

- Pro $0 &lt; \omega &lt; 1$ se n√°z√Ωv√° metodou doln√≠ relaxace. Je vhodn√° v p≈ô√≠padƒõ, kdy Gauss-Seidel nekonverguje.
- Pro $\omega = 1$ je toto≈æn√° s Gauss-Seidelem.
- Pro $\omega > 1$ se n√°z√Ωv√° metodou horn√≠ relaxace / SOR metodou. Zrychluje konvergenci Gauss-Seidela.

### Dekompozice matic

Metody podobn√© Gaussovƒõ eliminaci, ale s vlastnostmi, kter√© mohou b√Ωt vyhodn√©.

- **LU dekompozice**\
  Rozdƒõlen√≠ matice $A$ na horn√≠ doln√≠ troj√∫heln√≠kovou matici $L$ a horn√≠ troj√∫heln√≠kovou matici $U$, tak ≈æe $A = LU$.

  Je to v podstatƒõ Gaussova eliminace. Matice $P$ je permutaƒçn√≠ matice, kter√° prohazuje ≈ô√°dky:

  ```math
  P \cdot A = L \cdot U
  ```

  Plat√≠, ≈æe:

  ```math
  \begin{aligned}
  A \cdot x &= b \\
  A &= LU \\
  LU \cdot x &= b
  \end{aligned}
  ```

  P≈Øvodn√≠ probl√©m ≈ôe≈°en√≠ soustavy lin√°rn√≠ch rovnic se tedy p≈ôevede na dva probl√©my:

  ```math
  \begin{aligned}
  y &= U \cdot x \\
  L \cdot y &= b \\
  \end{aligned}
  ```

  ≈òe≈°√≠me tedy dva syst√©my rovnic s troj√∫heln√≠kov√Ωmi maticemi.

  Oproti Gaussovi je v√Ωhodnƒõj≈°√≠ pro:

  - Opakovan√© ≈ôe≈°en√≠ soustav s matic√≠ $A$ a r≈Øzn√Ωmi prav√Ωmi stranami $b$.
  - Inverzi matice $A$.
  - V√Ωpoƒçet determinantu matice $A$.

- **QR dekompozice**\
  Rozdƒõlen√≠ matice $A$ na ortogon√°ln√≠ matici $Q$ a horn√≠ troj√∫heln√≠kovou matici $R$ (u≈æ ne $U$), tak ≈æe $A = QR$.

  ```math
  \begin{aligned}

  A \cdot x &= b \\
  A = QR \Rightarrow U \cdot x &= Q^T \cdot b \\

  \end{aligned}
  ```

  Proto≈æe je ortogon√°ln√≠ a tedy $Q^{-1} = Q^T$.

  M√° lep≈°√≠ numerickou stabilitu ne≈æ LU dekompozice.

## Numerick√° diferenciace

Algoritmy numerick√© diferenciace (derivace) poƒç√≠taj√≠ odhady derivace re√°ln√Ωch funkc√≠ -- aproximuj√≠ $f'(x)$. Vyu≈æ√≠vaj√≠ p≈ôi tom zn√°m√© hodnoty t√©to funkce a jin√© znalosti a p≈ôedpoklady. [differentiation](#differentiation)

Numerick√° diferenciace se vyu≈æ√≠v√° pro aproximaci differenci√°ln√≠ch rovnic (p≈ôevodem na _diferenƒçn√≠ rovnice_).

- **Langrangeova interpolace**\
  Pokud zn√°me hodnoty $f$ m≈Ø≈æeme mezi nimi interpolovat pomoc√≠ Lagrangeova polynomu a derivovat ten, proto≈æe derivovat polynomy je jednoduch√©.

  **‚ùó IMPORTANT**\
   Lagrangeovu interpolaci ≈ôe≈°√≠ ƒç√°st ot√°zky [K≈ôivky a povrchy](../krivky-a-povrchy/).

- **Finite difference method**\
  Rodina metod numerick√© diferenciace, kter√© vyu≈æ√≠vaj√≠ _koneƒçn√© diference_. Tedy approximuj√≠ limitu v definici derivace mal√Ωmi posuny ve vstupn√≠ch hodnot√°ch diferenciovan√Ωch funkc√≠. [finite-difference-method](#finite-difference-method)

  Jednotliv√Ωm "odst√≠n≈Øm" -- konkr√©tn√≠m v√Ωpoƒçetn√≠m vzorc≈Øm -- t√©hle metody se ≈ô√≠k√° _diferenciaƒçn√≠ sch√©mata_.

  **üí° TIP**\
   Abych pravdu ≈ôekl, nepoda≈ôilo se mi naj√≠t zdroj pro konkr√©tn√≠ definici pojmu "diferenciaƒçn√≠ sch√©ma".

- **(Koneƒçn√©) diference prvn√≠ho ≈ô√°du / first-order (finite) differences**\
  Nejjednodu≈°≈°√≠ sch√©ma numerick√© diferenciace. Vych√°z√≠ z definice derivace. [finite-difference](#finite-difference)

  - _Dop≈ôedn√° diference / forward (finite) difference_

    ```math
    \frac{\partial f}{\partial x} \approx \frac{f(x+h) - f(x)}{h}
    ```

  - _Zpƒõtn√° diference / backward (finite) difference_

    ```math
    \frac{\partial f}{\partial x} \approx \frac{f(x) - f(x-h)}{h}
    ```

  - _Centr√°ln√≠ diference / central (finite) difference_

    ```math
    \frac{\partial f}{\partial x} \approx \frac{f(x+h) - f(x-h)}{2h}
    ```

  kde $h$ je kladn√© ƒç√≠slo napodobuj√≠c√≠ nekoneƒçnƒõ malou zmƒõnu (limitu) v definici derivace. M≈Ø≈æe to b√Ωt konstanta, m≈Ø≈æe ale b√Ωt i zvoleno adaptivnƒõ.

  **üí° TIP**\
   Teƒçna je tak napodobena seƒçnou.

- **Richardson extrapolation**\
  Zp≈Øsob zlep≈°en√≠ rate of convergence iterativn√≠ch metod. [richardson](#richardson)

## Zdroje

- [[[ma018,1]]] [MA018 Numerical Methods (podzim 2019)](https://is.muni.cz/auth/el/fi/podzim2019/MA018/)
- [[[numerical-analysis,2]]] [Wikipedia: Numerical analysis](https://en.wikipedia.org/wiki/Numerical_analysis)
- [[[root-finding,3]]] [Wikipedia: Root-finding algorithms](https://en.wikipedia.org/wiki/Root-finding_algorithms)
- [[[rate, 4]]] [Wikipedia: Rate of convergence](https://en.wikipedia.org/wiki/Rate_of_convergence)
- [[[regula-falsi,5]]] [Wikipedia: Regula falsi](https://en.wikipedia.org/wiki/Regula_falsi)
- [[[gauss-elimination,6]]] [Wikipedia: Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination)
- [[[jacobi-method,7]]] [Wikipedia: Jacobi method](https://en.wikipedia.org/wiki/Jacobi_method)
- [[[gauss-seidel,8]]] [Wikipedia: Gauss-Seidel method](https://en.wikipedia.org/wiki/Gauss%E2%80%93Seidel_method)
- [[[relaxation-method, 9]]] [Wikipedia: Relaxation (iterative method)](<https://en.wikipedia.org/wiki/Relaxation_(iterative_method)>)
- [[[differentiation, 10]]] [Wikipedia: Numerical differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation)
- [[[finite-difference, 11]]] [Wikipedia: Finite difference](https://en.wikipedia.org/wiki/Finite_difference)
- [[[finite-difference-method, 12]]] [Wikipedia: Finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method)
- [[[richardson,13]]] [Wikipedia: Richardson extrapolation](https://en.wikipedia.org/wiki/Richardson_extrapolation)
- [[[linear-eq, 14]]] [Wikipedia: System of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations)
- [[[numerical-stability, 15]]] [Wikipedia: Numerical stability](https://en.wikipedia.org/wiki/Numerical_stability)
- [[[numericka-metoda,16]]] [Wikipedia: Numerick√° metoda](https://cs.wikipedia.org/wiki/Numerick%C3%A1_metoda)
- [[[order-question,17]]] [What is the intuitive meaning of order of accuracy and order of approximation?](https://math.stackexchange.com/questions/2873291/what-is-the-intuitive-meaning-of-order-of-accuracy-and-order-of-approximation)
